<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>woods.lib.train_seq API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>woods.lib.train_seq</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import time
import numpy as np

import torch
from torch import nn, optim

from woods.lib import datasets
from woods.lib import models
from woods.lib import objectives
from woods.lib import hyperparams
from woods.lib import utils

## Train function
def train_step(model, loss_fn, objective, dataset, in_loaders_iter, optimizer, device):
    &#34;&#34;&#34;
    :param model: nn model defined in a models.py
    :param train_loader: training dataloader(s)
    :param optimizer: optimizer of the model defined in train(...)
    :param device: device on which we are training
    &#34;&#34;&#34;
    model.train()

    ts = torch.tensor(dataset.get_pred_time()).to(device)
        
    # Get next batch of training data 
    # TODO: Fix that awful patch
    try:
        batch_loaders = next(in_loaders_iter)
    except StopIteration:
        _, loaders = dataset.get_train_loaders()
        in_loaders_iter = zip(*loaders)
        batch_loaders = next(in_loaders_iter)

    # Send everything in an array
    minibatches_device = [(x, y) for x,y in batch_loaders]

    ## Group all inputs and send to device
    all_x = torch.cat([x for x,y in minibatches_device]).to(device)
    all_y = torch.cat([y for x,y in minibatches_device]).to(device)
    all_out = []

    # Get logit and make prediction
    all_out, pred = model(all_x, ts)

    # Compute environment-wise losses
    all_logits_idx = 0
    env_losses = torch.zeros(len(minibatches_device))
    for i, (x, y) in enumerate(minibatches_device):
        env_loss = 0
        y = y.to(device)
        for t_idx, out in enumerate(all_out):     # Number of time steps
            env_out_t = out[all_logits_idx:all_logits_idx + x.shape[0],:]
            env_loss += loss_fn(env_out_t, y[:,t_idx]) 
            objective.gather_logits_and_labels(env_out_t, y[:,t_idx])

        # get train accuracy and save it
        nb_correct = pred[all_logits_idx:all_logits_idx + x.shape[0],:].eq(y).cpu().sum()
        nb_items = pred[all_logits_idx:all_logits_idx + x.shape[0],:].numel()

        # Save loss
        env_losses[i] = env_loss

        # Update stuff
        all_logits_idx += x.shape[0]

    # Back propagate
    optimizer.zero_grad()
    objective.backward(env_losses)
    optimizer.step()

    return model

def train_seq_setup(flags, training_hparams, model, objective, dataset, device):

    loss_fn = nn.NLLLoss(weight=dataset.get_class_weight().to(device))
    optimizer = optim.Adam(model.parameters(), lr=training_hparams[&#39;lr&#39;], weight_decay=training_hparams[&#39;weight_decay&#39;])
    
    record = {}
    step_times = []
    
    t = utils.setup_pretty_table(flags)

    train_names, train_loaders = dataset.get_train_loaders()
    n_batches = np.sum([len(train_l) for train_l in train_loaders])
    train_loaders_iter = zip(*train_loaders)
    for step in range(1, dataset.N_STEPS + 1):

        ## Make training step and report accuracies and losses
        step_start = time.time()
        model = train_step(model, loss_fn, objective, dataset, train_loaders_iter, optimizer, device)
        step_times.append(time.time() - step_start)

        if step % dataset.CHECKPOINT_FREQ == 0 or (step-1)==0:

            val_start = time.time()
            checkpoint_record = get_accuracies_seq(model, loss_fn, dataset, device)
            val_time = time.time() - val_start

            record[str(step)] = checkpoint_record

            t.add_row([step] 
                    + [&#34;{:.2f} :: {:.2f}&#34;.format(record[str(step)][str(e)+&#39;_in_acc&#39;], record[str(step)][str(e)+&#39;_out_acc&#39;]) for e in dataset.get_envs()] 
                    + [&#34;{:.2f}&#34;.format(np.average([record[str(step)][str(e)+&#39;_loss&#39;] for e in train_names]))] 
                    + [&#34;{:.2f}&#34;.format((step*len(train_loaders)) / n_batches)]
                    + [&#34;{:.2f}&#34;.format(np.mean(step_times))] 
                    + [&#34;{:.2f}&#34;.format(val_time)])

            step_times = [] 
            print(&#34;\n&#34;.join(t.get_string().splitlines()[-2:-1]))

    return model, record

def get_accuracies_seq(model, loss_fn, dataset, device):

    # Get loaders and their names
    val_names, val_loaders = dataset.get_val_loaders()

    ## Get test accuracy and loss
    record = {}
    for name, loader in zip(val_names, val_loaders):
        accuracy, loss = get_split_accuracy(model, loss_fn, dataset, loader, device)
    
        record.update({name+&#39;_acc&#39;: accuracy,
                                name+&#39;_loss&#39;: loss})
    
    return record

def get_split_accuracy(model, loss_fn, dataset, loader, device):

    n_batch = 0
    losses = 0
    nb_correct = 0
    nb_item = 0

    ts = torch.tensor(dataset.get_pred_time()).to(device)

    model.eval()
    with torch.no_grad():

        for b, (data, target) in enumerate(loader):

            data, target = data.to(device), target.to(device)

            loss = 0
            all_out, pred = model(data, ts)

            for i, t in enumerate(ts):
                loss += loss_fn(all_out[i], target[:,i])

            nb_correct += pred.eq(target).sum()
            nb_item += pred.numel()
            losses += loss
            n_batch += 1

    return nb_correct.item() / nb_item, losses.item() / n_batch</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="woods.lib.train_seq.get_accuracies_seq"><code class="name flex">
<span>def <span class="ident">get_accuracies_seq</span></span>(<span>model, loss_fn, dataset, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_accuracies_seq(model, loss_fn, dataset, device):

    # Get loaders and their names
    val_names, val_loaders = dataset.get_val_loaders()

    ## Get test accuracy and loss
    record = {}
    for name, loader in zip(val_names, val_loaders):
        accuracy, loss = get_split_accuracy(model, loss_fn, dataset, loader, device)
    
        record.update({name+&#39;_acc&#39;: accuracy,
                                name+&#39;_loss&#39;: loss})
    
    return record</code></pre>
</details>
</dd>
<dt id="woods.lib.train_seq.get_split_accuracy"><code class="name flex">
<span>def <span class="ident">get_split_accuracy</span></span>(<span>model, loss_fn, dataset, loader, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_split_accuracy(model, loss_fn, dataset, loader, device):

    n_batch = 0
    losses = 0
    nb_correct = 0
    nb_item = 0

    ts = torch.tensor(dataset.get_pred_time()).to(device)

    model.eval()
    with torch.no_grad():

        for b, (data, target) in enumerate(loader):

            data, target = data.to(device), target.to(device)

            loss = 0
            all_out, pred = model(data, ts)

            for i, t in enumerate(ts):
                loss += loss_fn(all_out[i], target[:,i])

            nb_correct += pred.eq(target).sum()
            nb_item += pred.numel()
            losses += loss
            n_batch += 1

    return nb_correct.item() / nb_item, losses.item() / n_batch</code></pre>
</details>
</dd>
<dt id="woods.lib.train_seq.train_seq_setup"><code class="name flex">
<span>def <span class="ident">train_seq_setup</span></span>(<span>flags, training_hparams, model, objective, dataset, device)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_seq_setup(flags, training_hparams, model, objective, dataset, device):

    loss_fn = nn.NLLLoss(weight=dataset.get_class_weight().to(device))
    optimizer = optim.Adam(model.parameters(), lr=training_hparams[&#39;lr&#39;], weight_decay=training_hparams[&#39;weight_decay&#39;])
    
    record = {}
    step_times = []
    
    t = utils.setup_pretty_table(flags)

    train_names, train_loaders = dataset.get_train_loaders()
    n_batches = np.sum([len(train_l) for train_l in train_loaders])
    train_loaders_iter = zip(*train_loaders)
    for step in range(1, dataset.N_STEPS + 1):

        ## Make training step and report accuracies and losses
        step_start = time.time()
        model = train_step(model, loss_fn, objective, dataset, train_loaders_iter, optimizer, device)
        step_times.append(time.time() - step_start)

        if step % dataset.CHECKPOINT_FREQ == 0 or (step-1)==0:

            val_start = time.time()
            checkpoint_record = get_accuracies_seq(model, loss_fn, dataset, device)
            val_time = time.time() - val_start

            record[str(step)] = checkpoint_record

            t.add_row([step] 
                    + [&#34;{:.2f} :: {:.2f}&#34;.format(record[str(step)][str(e)+&#39;_in_acc&#39;], record[str(step)][str(e)+&#39;_out_acc&#39;]) for e in dataset.get_envs()] 
                    + [&#34;{:.2f}&#34;.format(np.average([record[str(step)][str(e)+&#39;_loss&#39;] for e in train_names]))] 
                    + [&#34;{:.2f}&#34;.format((step*len(train_loaders)) / n_batches)]
                    + [&#34;{:.2f}&#34;.format(np.mean(step_times))] 
                    + [&#34;{:.2f}&#34;.format(val_time)])

            step_times = [] 
            print(&#34;\n&#34;.join(t.get_string().splitlines()[-2:-1]))

    return model, record</code></pre>
</details>
</dd>
<dt id="woods.lib.train_seq.train_step"><code class="name flex">
<span>def <span class="ident">train_step</span></span>(<span>model, loss_fn, objective, dataset, in_loaders_iter, optimizer, device)</span>
</code></dt>
<dd>
<div class="desc"><p>:param model: nn model defined in a models.py
:param train_loader: training dataloader(s)
:param optimizer: optimizer of the model defined in train(&hellip;)
:param device: device on which we are training</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_step(model, loss_fn, objective, dataset, in_loaders_iter, optimizer, device):
    &#34;&#34;&#34;
    :param model: nn model defined in a models.py
    :param train_loader: training dataloader(s)
    :param optimizer: optimizer of the model defined in train(...)
    :param device: device on which we are training
    &#34;&#34;&#34;
    model.train()

    ts = torch.tensor(dataset.get_pred_time()).to(device)
        
    # Get next batch of training data 
    # TODO: Fix that awful patch
    try:
        batch_loaders = next(in_loaders_iter)
    except StopIteration:
        _, loaders = dataset.get_train_loaders()
        in_loaders_iter = zip(*loaders)
        batch_loaders = next(in_loaders_iter)

    # Send everything in an array
    minibatches_device = [(x, y) for x,y in batch_loaders]

    ## Group all inputs and send to device
    all_x = torch.cat([x for x,y in minibatches_device]).to(device)
    all_y = torch.cat([y for x,y in minibatches_device]).to(device)
    all_out = []

    # Get logit and make prediction
    all_out, pred = model(all_x, ts)

    # Compute environment-wise losses
    all_logits_idx = 0
    env_losses = torch.zeros(len(minibatches_device))
    for i, (x, y) in enumerate(minibatches_device):
        env_loss = 0
        y = y.to(device)
        for t_idx, out in enumerate(all_out):     # Number of time steps
            env_out_t = out[all_logits_idx:all_logits_idx + x.shape[0],:]
            env_loss += loss_fn(env_out_t, y[:,t_idx]) 
            objective.gather_logits_and_labels(env_out_t, y[:,t_idx])

        # get train accuracy and save it
        nb_correct = pred[all_logits_idx:all_logits_idx + x.shape[0],:].eq(y).cpu().sum()
        nb_items = pred[all_logits_idx:all_logits_idx + x.shape[0],:].numel()

        # Save loss
        env_losses[i] = env_loss

        # Update stuff
        all_logits_idx += x.shape[0]

    # Back propagate
    optimizer.zero_grad()
    objective.backward(env_losses)
    optimizer.step()

    return model</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="woods.lib" href="index.html">woods.lib</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="woods.lib.train_seq.get_accuracies_seq" href="#woods.lib.train_seq.get_accuracies_seq">get_accuracies_seq</a></code></li>
<li><code><a title="woods.lib.train_seq.get_split_accuracy" href="#woods.lib.train_seq.get_split_accuracy">get_split_accuracy</a></code></li>
<li><code><a title="woods.lib.train_seq.train_seq_setup" href="#woods.lib.train_seq.train_seq_setup">train_seq_setup</a></code></li>
<li><code><a title="woods.lib.train_seq.train_step" href="#woods.lib.train_seq.train_step">train_step</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>